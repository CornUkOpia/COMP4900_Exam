# COMP4900: Intro to Machine Learning
## The Language of Machine Learning
### Unsupervised
- Types of unsupervised machine learning include generative modeling, clustering, and anomaly detection.
#### Clustering
- Organizing data into clusters. Inferring distances between data points.
### Supervised
- Types of supervised machine learning include regression and classification.
#### Classification
- Predicts a finite categorical value belonging to a discrete set. (See Assignment 1, if the patient had hepatitis or didn't.)
#### Regression
- Predict a real continuous output value. Using the input features, predict the future result. 
### Input Variables
- Columns are called input variables or features or attributes.
### Output Variables
- Output variables / targets are the columns we are trying to predict. 
### Training example
- A row in the table is the training example / instance.
### Datasets
- The whole table containing all the training examples is the data set.
### Variable types
- Numerical: Real number measurements (Quantitative). 
- Categorical: From a discrete set (Qualitative)
- Ordinal: Categorical attributes that are ordered, allows ranking. (E.g: First, second, third)
### i.i.d assumption
- The independent and identically distributed assumption indicates that your data is independent and identically distributed. In supervised learning, the examples Xi in the training set are assumed to be independent and identically distributed.
- Independent: Every Xi is sampled according to some probability distribution D over the data domain X. For example, in an IQ study, if we measure an individual's IQ, it shouldn't provide any information about the next subject assessed. 
- Identically distributed: The distribution D is the same for all examples. One probability distribution should adequately model all values you observe in a sample. It is vital that the data is identically distributed as it indicates that you are are assessing a stable phenomenon. Example: If you are measuring the strength of a product and the mean strength increases as you collect more samples, it's hard to draw a conclusion. The mean strength depends on when you measure it. 
#### How to assess i.i.d in your dataset: 
- Independence: Consider how the data was collected for the data set. Was it randomly sampled or was it obtained in another way that would cause the information to be related to one another?
- Identical Distribution: Determine if there are any trends, graph the data in the order it was measured and look for patterns.
### Empiricial risk minimization
- Example for Explanation:
    - Problem: A simple supervised learning classification problem designed to classify spam emails. 
    - Each email has a label 0 (not spam) or 1 (spam). We denote the input variables with X and the output variable with Y.
    - The function f: X -> Y maps the input variables to the output variable.
    - Now that the problem has been defined, we need a model that is going to make our predictions. A synonym for model is the hypothesis H. The hypothesis is a function that takes input from X and produces an output label 0 or 1. This function h: X -> Y.
    - We want to find the hypothesis that minimizes our error, with this we come to the term empirical risk minimization. The term empirical implies that we minimize our error based on a sample set S from the domain set X.
    - Say we sample S from X, True error is based on the whole input X, however we only have access to S, a subset of X, we learn based on that sample of training examples. We don't have access to the true error, but to the empirical error.
    - The empirical error is also called generalisation error. In most problems, we don't have access to the whole X, instead S. We want to generalised based on S. This error is also called the risk. 
    - Since we have only S, it happens that we minimize the empirical error, but increase the true error. 
## Linear regression 
- The linear regression problem: : fw(xi) = w0 + ‚àëj=1:m Wj Xi,j where m is the number of features.
- The goal of linear regression is to find the best linear model given the training data. 
- Most common choice is to find the w that minimizes Err(w) (see Least Squares solution for this equation.)
### Least squares solution
- Find the best linear model given the data. 
- Most common choice is to find the w that minimizes squared error
    - Err(w) =  ‚àë i=1:n (Yi - w^T Xi)^2
    - Goal is to find a function in the form Fw(x) = w^T X
    - W is calculated such that the sum of squared error is minimized.
    - w = argmin ‚àë i=1:n (Yi - w^T Xi)^2
    - Yi (True target value)
    - w^T Xi (Predicted value for the target using the linear model and input variables.)
- Notation:
    - X is the n x m matrix of input data
    - Y is the n x 1 vector of output data
    - W is the m x 1 vector of weights
    - Fw(X) is the n x 1 vector of the predicted values
    - Err(w) is a scaler
    - In matrix notation
        - Fw(X) = X * w
        - Err(W) = (y - X * w)^T (y -  X * w)
    - wHat denotes the estimated weights that minimizes the error we defined.
    - ùúïErr(W)/ùúïw = -2X^T (y - X * w)
    - X^T (y - X * w) = 0
    - X^T * y - X^T * w = 0
    - wHat = (X^T * X)^-1 * (X^T * y)
### Predicting new data
### Failure modes
### Singularities
Bad fits
Input variables for linear regression
Order-nth fit
Overfitting/Generalisation
Uses of data
Training
Validation
Test
K-fold cross validation
Cross validation for comparing models
Leave one out cross validation
Linear Classification
Classification Problems
Classification via linear regression
High level views of binary classification
Probabilistic 
Discriminative Learning
Probabilistic view of discriminative learning
Discriminative learning logistic regression
Learning the weights
Maximizing likelihood
Gradient Descent
Classification of a new observation
Generative learning
Decision Boundaries
Linear Discriminant Analysis
Higher-order features
Quadratic Discriminant Analysis
Scaling up Generative Learning
Naive Bayes
Assumptions
Training
Bernoulli
Text classification
Laplace Smoothing
Evaluation, Bias-Variance, and Regularization
Preprocessing raw data
Multi-class classification
Performance
Terminology
Common measures
Receiver operating characteristics
Area under curve
Bias and variance in Machine learning
Ridge Regression (L2-Regularisation)
Lasso Regression (L1-Regularisation)
Ridge vs. Lasso
Decision Trees
Linear assumptions
Beyond linear models
Decision Trees
Example with Discrete Input
Expressiveness
Classification and regression
How do we learn a decision tree?
Choosing a good split attribute
Quantifying Uncertainty
Entropy
Of a joint distributions
Conditional
Information gain
Constructing decision trees
Attribute selection
Compare trees
What makes a good tree?
Avoiding overfitting
Advantages
Feature construction
Steps to solving a supervised learning problem
Encoding input into a feature vector
Words
TF-IDF
N_grams
Dimension Reduction:
Feature extraction
Feature selection
Methods
Wrapper and Filter
Embedded
Variable Ranking
Principal Component Analysis
Directions of largest variances.
Scoring functions
Nonlinear dependencies with MI
Best-subset selection
Instance Based Learning
Nearest neighbour
K-Nearest neighbour
Limitations
Distance-weighted NN
Gaussian Weighting
Ensemble Methods
Bagging
Boosting
Stacking
Weak Learners
AdaBoost
Meta-model
Neural Networks
Perceptron: A simple linear classifier
Feed-forward neural networks
Learning
Generalising
Fully connected networks
Gradient descent 
Preliminaries for NN
Update for the output node
Update for the hidden node
Stochastic gradient descent
Organizing training data
Activation Functions
Sigmoid
Tanh
ReLu
Regularisation
Compute gradients
Overtraining
Choosing the learning rate
Adaptive optimization algorithms
Convolutional Neural Networks
NNs to CNNs
Convolution as Dot Product
Detecting virtual edges
Zero padding
Stride
Convolution on RGB
Two Filters
Pooling
Dropout Regularisation
Softmax
LeNet-5
AlexNet
VGG-16
Recurrent Neural Networks
Motivation
Forward Pass
Backpropagation
Long-term dependencies
Problem of long-term dependencies
Long short-term memory (LSTM) units
Bidirectional RNN
