# COMP4900: Intro to Machine Learning
## The Language of Machine Learning
### Unsupervised
- Types of unsupervised machine learning include generative modeling, clustering, and anomaly detection.
#### Clustering
- Organizing data into clusters. Inferring distances between data points.
### Supervised
- Types of supervised machine learning include regression and classification.
#### Classification
- Predicts a finite categorical value belonging to a discrete set. (See Assignment 1, if the patient had hepatitis or didn't.)
#### Regression
- Predict a real continuous output value. Using the input features, predict the future result. 
### Input Variables
- Columns are called input variables or features or attributes.
### Output Variables
- Output variables / targets are the columns we are trying to predict. 
### Training example
- A row in the table is the training example / instance.
### Datasets
- The whole table containing all the training examples is the data set.
### Variable types
- Numerical: Real number measurements (Quantitative). 
- Categorical: From a discrete set (Qualitative)
- Ordinal: Categorical attributes that are ordered, allows ranking. (E.g: First, second, third)
### i.i.d assumption
- The independent and identically distributed assumption indicates that your data is independent and identically distributed. In supervised learning, the examples Xi in the training set are assumed to be independent and identically distributed.
- Independent: Every Xi is sampled according to some probability distribution D over the data domain X. For example, in an IQ study, if we measure an individual's IQ, it shouldn't provide any information about the next subject assessed. 
- Identically distributed: The distribution D is the same for all examples. One probability distribution should adequately model all values you observe in a sample. It is vital that the data is identically distributed as it indicates that you are are assessing a stable phenomenon. Example: If you are measuring the strength of a product and the mean strength increases as you collect more samples, it's hard to draw a conclusion. The mean strength depends on when you measure it. 
#### How to assess i.i.d in your dataset: 
- Independence: Consider how the data was collected for the data set. Was it randomly sampled or was it obtained in another way that would cause the information to be related to one another?
- Identical Distribution: Determine if there are any trends, graph the data in the order it was measured and look for patterns.
### Empiricial risk minimization
- Example for Explanation:
    - Problem: A simple supervised learning classification problem designed to classify spam emails. 
    - Each email has a label 0 (not spam) or 1 (spam). We denote the input variables with X and the output variable with Y.
    - The function f: X -> Y maps the input variables to the output variable.
    - Now that the problem has been defined, we need a model that is going to make our predictions. A synonym for model is the hypothesis H. The hypothesis is a function that takes input from X and produces an output label 0 or 1. This function h: X -> Y.
    - We want to find the hypothesis that minimizes our error, with this we come to the term empirical risk minimization. The term empirical implies that we minimize our error based on a sample set S from the domain set X.
    - Say we sample S from X, True error is based on the whole input X, however we only have access to S, a subset of X, we learn based on that sample of training examples. We don't have access to the true error, but to the empirical error.
    - The empirical error is also called generalisation error. In most problems, we don't have access to the whole X, instead S. We want to generalised based on S. This error is also called the risk. 
    - Since we have only S, it happens that we minimize the empirical error, but increase the true error. 
## Linear regression 
- The linear regression problem: : fw(xi) = w0 + ‚àëj=1:m Wj Xi,j where m is the number of features.
- The goal of linear regression is to find the best linear model given the training data. 
- Most common choice is to find the w that minimizes Err(w) (see Least Squares solution for this equation.)
### Least squares solution
- Find the best linear model given the data. 
- Most common choice is to find the w that minimizes squared error
    - Err(w) =  ‚àë i=1:n (Yi - w^T Xi)^2
    - Goal is to find a function in the form Fw(x) = w^T X
    - W is calculated such that the sum of squared error is minimized.
    - w = argmin ‚àë i=1:n (Yi - w^T Xi)^2
    - Yi (True target value)
    - w^T Xi (Predicted value for the target using the linear model and input variables.)
- Notation:
    - X is the n x m matrix of input data
    - Y is the n x 1 vector of output data
    - W is the m x 1 vector of weights
    - Fw(X) is the n x 1 vector of the predicted values
    - Err(w) is a scaler
    - In matrix notation
        - Fw(X) = X * w
        - Err(W) = (y - X * w)^T (y -  X * w)
    - wHat denotes the estimated weights that minimizes the error we defined.
    - ùúïErr(W)/ùúïw = -2X^T (y - X * w)
    - X^T (y - X * w) = 0
    - X^T * y - X^T * w = 0
    - wHat = (X^T * X)^-1 * (X^T * y)
    - Best Fitted model: yHat = X * wHat
### Predicting new data
    - To predict the outcome of new data Xnew -> Ynew
    - yHatNew = Xnew * wHat
### Gradient Descent
- Gradient descent is an iterative optimisation algorithm to find the minimum of a function.
- We want to produce a sequence of weight solutions w0,w1,w2,..., such that: Err(W0) > Err(W1) > Err(W2) > ...
- The algorithm: 
    - Given an initial weight vector w0,
    - Do for k = 0,1,2,...
        - calculate Z = ùúïErr(Wk)/ùúïWk
        - Wk+1 = Wk - Ak * Z (Take a step in the negative direction specified by the gradient)
    - End when || Wk+1 - Wk || < E
- Parameter Ak > 0 is the step size (or learning rate) for iteration k.
- Gradient descent steps down the function the direction of the steepest descent. The size of each step is determined the learning rater.
- In the gradient descent algorithm, one can infer that...
    - If the slope is positive, the value of the weights of the hypothesis decreases.
    - If the slope is negative, the value of the weights of the hypothesis increases.
- Failure in gradient descent. 
    - If Ak is too large, the gradient descent can overshoot the minimum and fail to converge or diverge.
    - If Ak is too small, the gradient descent will take small steps to reach the local minima and it will take a long time.
### Failure modes
#### Avoid Singularities
- The weights are not uniquely defined. Example: One feature is a linear function of the other. Solved by re-coding or dropping redundant columns of X.
- The number of features (m) exceeds the number of training examples (n). Solved by reducing the number of features using various techniques. 
### Bad fits
- Linear regression can be a bad fit in some cases.
- In order to improve the fit...
    - Pick a better function
    - Use more features
    - Get more data.
### Input variables for linear regression
- Original quantitative variables: X1,..., Xm
- Transformations of variables: e.g Xm+1 = log(Xi)
- Basis expansions: e.g. Xm+1 = Xi^2, Xm+2 = Xi^3
- Interaction terms: e.g. Xm+1 = XiXj
- In all cases, we can add Xm+1,...,Xm+k to the list of original variables and perform the linear regression.
### Order-nth fit
- Order-1 polynomial: Ex: y = 1.6x + 1.05
- Order-2 polynomial: Ex: y = 0.68x^2 + 1.74x + 0.73
- Constructing a curve that has the best fit to a series of data points, subject to constraints. The n value is the number of points to which the curve will be fitted. 
### Overfitting/Generalisation
- We can find a model that explains perfectly, the training data, but does not generalise well to new data.
#### Overfitting: 
- We can do really well in the training data, but it doesn't generalise to unseen data. We don't want to just minimize error on the training set. We want to minimize the true error and develop models that generalise well to unseen data. 
- Every model has a true error measured on all possible data items we could ever encounter, but we don't have all possible data. In order to decide what is a good model, we measure error over the training set. 
- Example: Suppose we compare 2 models F and G trained using the same algorithm, assume F has a lower error on the training set and G has lower true error, then our algorithm is overfitting.
#### Generalisation:
- In ML, our goal is to develop models that generalise.
- Overly simple model:
    - High training error and high test error.
- Overly complex model:
    - Low training error but high test error.
### Uses of data
- Paritioning the data into 3 sets help to evaluate the data.
#### Training set
- Used to fit a model (find the best hypothesis in the class; learn parameters)
#### Validation set
- Used for model selection, i.e., to estimate true error and compare hypothesis classes. (E.g., compare different order polynomials). 
- Validation set is used to compare different models during development.
    - Compare hypothesis/model classes. E.g., should I use a first- or second-order polynomial fit?
- Compare hyperparameters (i.e., a parameter that is not learned but that could impact performance). E.g., what learning rate should I use?
#### Test set
- What you report the final accuracy on.
### K-fold cross validation
- Instead of just one validation set, we can evaluate on many splits!
    - Consider k partitions of the training/non-test data (usually of equal size).
    - Train with k-1 subsets, validate on kth subset. Repeat k times.
    - Average the prediction error over the k rounds/folds.
### Cross validation for comparing models
- Say we have 2 models and this is a 5-fold cross validation
- Model/hypothesis A: Linear regression with basic features 
    - ErrA = 1/5 ‚àëj=1:5 Err^A_I (Average error of model A across all folds.)
- Model/hypothesis B: Linear regression with quadratic expansions of features
    - ErrB = 1/5 ‚àëj=1:5 Err^B_I (Average error of model B across all folds.)
- If ErrA < ErrB then we should use Model A, we should use a linear regression model with basic features.  
- Then to get the final Model A weights, we train on the entire dataset, ignoring the cross validation folds.
### Leave one out cross validation
- Let k = n, the size of the training set
- For each model / hyperparameter setting,
    - Repeat n times:
        - Set aside one instance (xi, yi) from the training set.
        - Use all other data points to find w (optimization).
        - Measure prediction error on the held-out (xi, yi).
    - Average the prediction error over all n subsets.
- Choose the setting with lowest estimated true prediction error.

## Linear Classification
### Classification Problems
- Y is a finite discrete set
- Given data set D = (Xi, Yi), with Yi, find a hypothesis which best fits the data.
    - If Yi = (0,1), this is a binary classification.
    - If Yi can take more than two values, the problems is called multi-class classifications.
### Classification via linear regression
- Example: A tumor classifier.
    - Input: X - Tumor size
    - Binary output:
        - Y = {No recurrence = 0; Recurrence = 1}
    - yHatn = Xn (X^T * X)^-1 * X^T * Y
    - How to get a binary output?
        - Threshold: If yHat > threshold then norecurrence class, if yHat <= then Recurrence class.
        - Interpret output as probability: yHat = Probability (Recurrence)
### Probabilistic 
- Estimate P(y | x), the conditional probability of the target variable given the feature data.
### Discriminative Learning
- Partition the feature space into different regions, and classify points based on the region where they lie.
## Logistic Regression
### Probabilistic view of discriminative learning
- Suppose we have 2 classes: y = (0,1)
- What is the probability of class y = 1 given input X?
- Logistic Function = 1/ 1+exp(-a)
- Log-odds ratio = ln ((P(y = 1 | x))/ P(y = 0 | x))
### Discriminative learning
- Model the boundary between the different classes.
- a = Log-odds ratio = W0 + W1X1 + ... WmXm
- The decision boundary is the set of points for which the linear model predicts zero. a = 0.
    - In fact, we model decision boundary with linear model.
    - a = 0 means Class 1 is equally likely as Class 2.
    - If a > 0, Class 1 is more likely than Class 2.
    - If a < 0, Class 2 is more likely  than Class 1.
    - pHat(y = 1 | x) = ùúï(w^T * X) = 1 / (1 + e^-W^T*X)
    - Logistic regression has a decision boundary that is linear in X.
### Learning the weights
- For y = (0,1), the likelihood function is
    - L(D) = P(y1,...,yn|x1,...,xn,w)
    - The likelihood of the data L(D) = Probability of correctly classifying training the data given the model parameters
        - If Yi = 1, = ùúï(W^T * Xi)
        - If Yi = 0, = 1 - ùúï(W^T * Xi)
### Maximizing likelihood
- Our goal is to maximize the likelihood, we want to find the parameters than give the highest likelihood.
- Likelihood: L(D) =  ‚àëi=1:n ùúï(W^T * Xi)^Yi (1 - ùúï(W^T * Xi))^ 1 - Yi
    - Problem: Taking products of lots of small numbers is numerically unstable, making this function hard to optimise.
- Log likelihood: L(D) = ln(L(D)) = ‚àëi=1:n Yi * ln(ùúï(w^T * Xi)) + (1 - Yi) * ln(1 - ùúï(w^T * Xi))
- The negative log-likelihood of the logistic function is known as the cross-entropy loss.
- cross-entropy(D) = - ‚àëi=1:n Yi * ln(ùúï(w^T * Xi)) + (1 - Yi) * ln(1 - ùúï(w^T * Xi))
    - Basic idea: It measures how many bits of information we would need to correct the errors made by our model. 
- In cross validation, look at error metrics on the validation set, not loss.
### Gradient Descent
- ùúïErr(W)/ùúïw = - ‚àëi=1:n Xi (Yi - ùúï(w^T * Xi))
    - Yi = True output.
    - ùúï(w^T * Xi) = Predicted output.
- Update rule: Wk+1 = Wk + aK * ‚àëi=1:n Xi (Yi - ùúï(w^T * Xi))
- Move in a direction that makes our prediction better.
- The algorithm:
- Given an initial weight vector w0,
- Do for k=0,1,2,...
    - Calculate T = ùúïErr(W)/ùúïw = - ‚àëi=1:n Xi (Yi - ùúï(w^T * Xi))
    - wK+1 = wK - aK * T (Take a step in the negative direction specified by the gradient.)
- End when ||wK+1 - wK|| < E
### Classification of a new observation
- After estimating w using gradient descent, calculate probabilities for a given new feature vector Xnew as follows:
    - p(yNew = 1 | xNew) = ùúï(w^T * xNew)
    - p(yNew = 0 | xNew) = 1 - ùúï(w^T * xNew)
- If p(yNew = 1 | xNew) > p(yNew = 0 | xNew) then yNew = 1 (xNew belongs to class 1)
- If p(yNew = 0 | xNew) > p(yNew = 1 | xNew) then yNew = 0 (xNew belongs to class 0)
### Generative learning
- Model the distribution of the different class.
- Separately model P(x|y) and P(y). Use Bayes' rule, to estimate P(y|x):
- P(y = 1|x) = (P(x|y = 1) * P(y = 1)) / P(x)
    - P(y = 1 | x) (The conditional probability of the target class (Our prediction))
    - P(x | y = 1) (How likely are we to see the observed features if the point was from class 1?)
        - Usually hard to estimate.
    - P(y = 1) (What is the marginal probability of this class? Ignoring the features, how likely are we to see class 1?)
    - P(x) (What is the marginal probability of the observed features? This is independent of the class.)
    - P(y) (Marginal probability of the class wihthot considering the features)
        - Usually easy to estimate.
- Example from spam classification: 
    - P(y = 1) = 0.01 -> In general, 1% of emails are spam.
    - P(x|y = 1) = 50% -> 50% chance of observed features occurring in a spam email.
    - P(x) = 0.10 -> 10% chance of seeing the observed features in a random email
    - P(y = 1|x) = 0.05 -> 5% chance of the email being spam.
- Why is P(x|y) and P(y) modeled seperately in Generative learning?
    - It gives extra flexibility. In the context of the spam email, suppose the spammers become more active with up to 20% of emails being spam. All that is needed to be done is modifying P(y).
    - Modeling P(x|y) allows us to make structural assumptions about the data generating process.
    - Generative models work well with smaller datasets.
### Linear Discriminant Analysis
- Linear Discriminant Analysis is a dimensionality reduction technique.     
    - LDA reduces the number of dimensions in a dataset while retaining as much information as possible. 
    - LDA uses the information from all features to create a new axis and projects the data on to the new axis in such a way as to minimizes the variance and maximizes the distance between the means of the two classes.
- LDA makes Gaussian assumptions about P(x|y)
- LDA is used for classification.
- The number of parameters to estimate in LDA is more than the number of parameters in Logistic Regression.
- LDA makes the assumptions that your data is Gaussian and that each attribute has the same variance. 
- Every class is assumed to be a Gaussian/normally distributed cluster of data points. 
- P(x|y=0) and P(x|y=1) are assumed to have the same covariance matrix. 
- LDA supports binary and multi-class classifications
#### Limations of Logistic Regression
- Two-Class Problems: Logistic regression is intended for two-classes or binary classification problems.
- Unstable with few examples / seperated classes.
### Higher-order features
- To get more flexible (non-linear) decision boundaries. Use higher-order features.
- X1,X2 are linear.
- X1,X2,X1X2,X1^2, X2^2 are curved and give a more flexible decision boundary.  
### Quadratic Discriminant Analysis
- LDA assumes all classes have the same covariance matrix.
- In QDA, each class uses its own estimate of variance or covariance where there are multiple input variables
- QDA allows different covariance matrices for each class k.
- QDA has more parameters to estimate, but greater flexibility to estimate the target function with a risk of overfitting. 
- QDA is more accurate than LDA. 
- LDA is faster than QDA due to the complex matrix operations and training time. 
- LDA is easier to analyse. 
## Naive Bayes
- In NB, assume that all of the columns of the dataset (Xj , j=1,...,m) are conditionally independent given y.
    - P(Xj | y) = P(Xj | y, Xk) for all j,k.
- Generative model structure for a data point x
    - P(x|y) = P(x1,x2,...,xm|y) = P(x1|y)P(x2|y)...P(Xm|y)
### Training
- Training a Naive Bayes classifier entails maximizing the log-likelihood function
- Likelihood: L(D) = P(Y1,Y2,...Yn|X1,X2,...,Xn)
    - Because Samples i=1:n are independent so we take product over n.
    - Input features are independent.
### Bernoulli
- A variant of Naive Bayes
- Prediction for a new input data x using log-odds ratio:
    - a(x) = log(P(y=1|x)/P(y=0|x)) = log((P(x|y=1)P(y=1)/P(x|y=0)P(y=0)))
    - If a(x) > 0, then classify as 1, If a(x) < 0, then classify as 0.
    - a(x) gives the decision boundary. 
        - Since this decision boundary corresponds to the log-odds, we can calso compute the class probability
        - Has a linear decision boundary.
### Laplace Smoothing
- Used in text classification to handle words that aren't observed in the training data, as running the maximum likelihood Naive Bayes model would result in the test document with the previously unseen word having a probability of 0.
- Replace the maximum likelihood estimate
    - ùõ≥j,1 = P(Xj|y=1) = (number of instances with Xj = 1 and Y = 1) / (number of examples with Y = 1) with ùõ≥j,1 = P(Xj|y=1) = ((number of instance with Xj = 1 and Y=1) + 1)/((Number of examples with y=1)+2)
    - If no example from that class, it reduces to a prior probability Pr=1/2.
    - If all examples have Xj = 1, then P(Xj = 0|y) has P = 1 / (# of examples + 2)
    - If a word appears frequently, the new estimate is only slightly biased.
    - You should laplace smooth both P(Xj |y=1) = ùõ≥j,1 and P(Xj | y=0) = ùõ≥j,0
## Evaluation, Bias-Variance, and Regularization
### Preprocessing raw data
- Normalisation across different features (z-score)
    - Centering and scaling with Xj^' = (ùë•ùëó ‚Äì ùúáùëó) / 6j
### Multi-class classification
- For a k-way classification problem, there are generally 2 options
    - Option 1: Learn a single classifier thaqt can produce k distinct output values.
        -   For Naive Bayes, compute P(y|x) for each class and select the class with the highest probability (See assignment 2 and the subreddit classification)
    - Option 2: Learn k different 1-vs-all binary classifiers
        - Applies to all binary classifiers, so more flexible, but often slower and creates an imbalance problem as the target class has relatively fewer data points, compared to the aggregation of the other classes.
### Performance
- Not all errors have equal impact.
- Typical classification errors:
    - Example: Consider the spam email classifier
        - A message that is not spam is assigned to the spam folder (Type 1 error or false positive)
        - A message that is spam appears in the regular folder (Type 2 error or false negative)
### Terminology
- Type of classification outputs:
    - True positive (m11): Example of class 1 predicted as class 1.
    - True negative (m00): Example of class 0 predicted as class 0.
    - False positive (m01): Example of class 0 predicted as class 1. Type I error.
    - False negative (m10): Example of class 1 predicted as class 0. Type II error.
- Total number of instances:
      - ntest = m00 + m01 + m10 + m11
### Common measures
- Accuracy = (TP + TN) / (TP + FP + FN + TN)
    - Error = 1 - Accuracy
- Precision = TP/(TP + FP) Total number of declared positives
- Recall = TP / (TP + FN) Total number of actual positives
- Sensitivity is the same as recall.
- Specificity = TN / (FP + TN) Total number of actual negatives
- False Positive Rate = FP / (FP + TN)
- True Positive is the same as recall.
- F1 measure: F = 2 ((Precision * Recall)/ (Precision + Recall)) (F is harmonic mean of precision and recall.)
### Receiver operating characteristics
- Often have a trade-off between false positives and false negatives.
    - Note: True Positive Rate = 1 ‚Äì False Negative rates.
    - It is common to plot ROC curves as TPR versus FPR. 
- To build the ROC curve:
    1. Train a classifier.
    2. Vary the decision boundary threshold.
    3. Compute FP rate and TP rate for different decision boundaries associated to the thresholds.
### Area under curve
- To compare 2 algorithms over a range of classification thresholds, consider the Area Under the (ROC) Curve (AUC).
    - A perfect algorithm has AUC=1.
    - A random algorithm has AUC=0.5.
    - Higher AUC doesn‚Äôt mean all performance measures are better.
### Bias and variance in Machine learning
- Training Set error vs. Validation set error 
- 1%           10%          High variance
- 15%          16%          High bias
- 15%          25%          High bias and high variance
- 1%            2%          Low bias and low variance

- There are two major sources of error in machine learning
    - Bias and variance.
- In machine learning, we informally think of
    - The difference between training and validation error as algorithm‚Äôs variance.
    - The difference between the Bayes error and the train error as algorithm‚Äôs bias.
        - Bayes error is the lowest possible prediction error that can be achieved.
- The field of statistics has more formal definitions of bias and variance.
    - If your error metric is mean squared error
        - You can write down formulas specifying these two quantities, and prove that 
            - Total Error = Bias + Variance.
        - But we don‚Äôt use this. The more informal definition of bias and variance given above will suffice.\
- High bias (Training data performance): Try a more complex model.
- High variance (Validation set performance): Try a simpler model, get more data, or regularization.
### Ridge Regression (L2-Regularisation)
- Constrains the weights by imposing a penalty on their size:
    - wHatRidge = argminW    { ‚àëi=1:n(Yi - w^T * Xi)^2 + Œª‚àëj=0:m Wj^2}
- The regularisation parameter Œª can be selected manually or by cross validation.
### Lasso Regression (L1-Regularisation)
### Ridge vs. Lasso
### Decision Trees
### Linear assumptions
### Beyond linear models
### Decision Trees
### Example with Discrete Input
### Expressiveness
### Classification and regression
### How do we learn a decision tree?
### Choosing a good split attribute
### Quantifying Uncertainty
### Entropy
### Of a joint distributions
### Conditional
### Information gain
### Constructing decision trees
### Attribute selection
### Compare trees
### What makes a good tree?
### Avoiding overfitting
### Advantages
### Feature construction
### Steps to solving a supervised learning problem
### Encoding input into a feature vector
### Words
### TF-IDF
### N_grams
### Dimension Reduction:
### Feature extraction
### Feature selection
### Methods
### Wrapper and Filter
### Embedded
### Variable Ranking
### Principal Component Analysis
### Directions of largest variances.
### Scoring functions
### Nonlinear dependencies with MI
### Best-subset selection
### Instance Based Learning
### Nearest neighbour
### K-Nearest neighbour
### Limitations
### Distance-weighted NN
### Gaussian Weighting
### Ensemble Methods
### Bagging
### Boosting
### Stacking
### Weak Learners
### AdaBoost
### Meta-model
### Neural Networks
### Perceptron: A simple linear classifier
### Feed-forward neural networks
### Learning
### Generalising
### Fully connected networks
### Gradient descent 
### Preliminaries for NN
### Update for the output node
### Update for the hidden node
### Stochastic gradient descent
### Organizing training data
### Activation Functions
### Sigmoid
### Tanh
### ReLu
### Regularisation
### Compute gradients
### Overtraining
### Choosing the learning rate
### Adaptive optimization algorithms
### Convolutional Neural Networks
### NNs to CNNs
### Convolution as Dot Product
### Detecting virtual edges
### Zero padding
### Stride
### Convolution on RGB
### Two Filters
### Pooling
### Dropout Regularisation
### Softmax
### LeNet-5
### AlexNet
### VGG-16
### Recurrent Neural Networks
### Motivation
### Forward Pass
### Backpropagation
### Long-term dependencies
### Problem of long-term dependencies
### Long short-term memory (LSTM) units
### Bidirectional RNN
