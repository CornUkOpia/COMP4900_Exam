# COMP4900: Intro to Machine Learning
## The Language of Machine Learning
### Unsupervised
- Types of unsupervised machine learning include generative modeling, clustering, and anomaly detection.
#### Clustering
- Organizing data into clusters. Inferring distances between data points.
### Supervised
- Types of supervised machine learning include regression and classification.
#### Classification
- Predicts a finite categorical value belonging to a discrete set. (See Assignment 1, if the patient had hepatitis or didn't.)
#### Regression
- Predict a real continuous output value. Using the input features, predict the future result. 
### Input Variables
- Columns are called input variables or features or attributes.
### Output Variables
- Output variables / targets are the columns we are trying to predict. 
### Training example
- A row in the table is the training example / instance.
### Datasets
- The whole table containing all the training examples is the data set.
### Variable types
- Numerical: Real number measurements (Quantitative). 
- Categorical: From a discrete set (Qualitative)
- Ordinal: Categorical attributes that are ordered, allows ranking. (E.g: First, second, third)
### i.i.d assumption
- The independent and identically distributed assumption indicates that your data is independent and identically distributed. In supervised learning, the examples Xi in the training set are assumed to be independent and identically distributed.
- Independent: Every Xi is sampled according to some probability distribution D over the data domain X. For example, in an IQ study, if we measure an individual's IQ, it shouldn't provide any information about the next subject assessed. 
- Identically distributed: The distribution D is the same for all examples. One probability distribution should adequately model all values you observe in a sample. It is vital that the data is identically distributed as it indicates that you are are assessing a stable phenomenon. Example: If you are measuring the strength of a product and the mean strength increases as you collect more samples, it's hard to draw a conclusion. The mean strength depends on when you measure it. 
#### How to assess i.i.d in your dataset: 
- Independence: Consider how the data was collected for the data set. Was it randomly sampled or was it obtained in another way that would cause the information to be related to one another?
- Identical Distribution: Determine if there are any trends, graph the data in the order it was measured and look for patterns.
### Empiricial risk minimization
- Example for Explanation:
    - Problem: A simple supervised learning classification problem designed to classify spam emails. 
    - Each email has a label 0 (not spam) or 1 (spam). We denote the input variables with X and the output variable with Y.
    - The function f: X -> Y maps the input variables to the output variable.
    - Now that the problem has been defined, we need a model that is going to make our predictions. A synonym for model is the hypothesis H. The hypothesis is a function that takes input from X and produces an output label 0 or 1. This function h: X -> Y.
    - We want to find the hypothesis that minimizes our error, with this we come to the term empirical risk minimization. The term empirical implies that we minimize our error based on a sample set S from the domain set X.
    - Say we sample S from X, True error is based on the whole input X, however we only have access to S, a subset of X, we learn based on that sample of training examples. We don't have access to the true error, but to the empirical error.
    - The empirical error is also called generalisation error. In most problems, we don't have access to the whole X, instead S. We want to generalised based on S. This error is also called the risk. 
    - Since we have only S, it happens that we minimize the empirical error, but increase the true error. 
## Linear regression 
- The linear regression problem: : fw(xi) = w0 + âˆ‘j=1:m Wj Xi,j where m is the number of features.
- The goal of linear regression is to find the best linear model given the training data. 
- Most common choice is to find the w that minimizes Err(w) (see Least Squares solution for this equation.)
### Least squares solution
- Find the best linear model given the data. 
- Most common choice is to find the w that minimizes squared error
    - Err(w) =  âˆ‘ i=1:n (Yi - w^T Xi)^2
    - Goal is to find a function in the form Fw(x) = w^T X
    - W is calculated such that the sum of squared error is minimized.
    - w = argmin âˆ‘ i=1:n (Yi - w^T Xi)^2
    - Yi (True target value)
    - w^T Xi (Predicted value for the target using the linear model and input variables.)
- Notation:
    - X is the n x m matrix of input data
    - Y is the n x 1 vector of output data
    - W is the m x 1 vector of weights
    - Fw(X) is the n x 1 vector of the predicted values
    - Err(w) is a scaler
    - In matrix notation
        - Fw(X) = X * w
        - Err(W) = (y - X * w)^T (y -  X * w)
    - wHat denotes the estimated weights that minimizes the error we defined.
    - ğœ•Err(W)/ğœ•w = -2X^T (y - X * w)
    - X^T (y - X * w) = 0
    - X^T * y - X^T * w = 0
    - wHat = (X^T * X)^-1 * (X^T * y)
    - Best Fitted model: yHat = X * wHat
### Predicting new data
- To predict the outcome of new data Xnew -> Ynew
- yHatNew = Xnew * wHat
### Gradient Descent
- Gradient descent is an iterative optimisation algorithm to find the minimum of a function.
- We want to produce a sequence of weight solutions w0,w1,w2,..., such that: Err(W0) > Err(W1) > Err(W2) > ...
- The algorithm: 
    - Given an initial weight vector w0,
    - Do for k = 0,1,2,...
        - calculate Z = ğœ•Err(Wk)/ğœ•Wk
        - Wk+1 = Wk - Ak * Z (Take a step in the negative direction specified by the gradient)
    - End when || Wk+1 - Wk || < E
- Parameter Ak > 0 is the step size (or learning rate) for iteration k.
- Gradient descent steps down the function the direction of the steepest descent. The size of each step is determined the learning rater.
- In the gradient descent algorithm, one can infer that...
    - If the slope is positive, the value of the weights of the hypothesis decreases.
    - If the slope is negative, the value of the weights of the hypothesis increases.
- Failure in gradient descent. 
    - If Ak is too large, the gradient descent can overshoot the minimum and fail to converge or diverge.
    - If Ak is too small, the gradient descent will take small steps to reach the local minima and it will take a long time.
### Failure modes
#### Avoid Singularities
- The weights are not uniquely defined. Example: One feature is a linear function of the other. Solved by re-coding or dropping redundant columns of X.
- The number of features (m) exceeds the number of training examples (n). Solved by reducing the number of features using various techniques. 
### Bad fits
- Linear regression can be a bad fit in some cases.
- In order to improve the fit...
    - Pick a better function
    - Use more features
    - Get more data.
### Input variables for linear regression
- Original quantitative variables: X1,..., Xm
- Transformations of variables: e.g Xm+1 = log(Xi)
- Basis expansions: e.g. Xm+1 = Xi^2, Xm+2 = Xi^3
- Interaction terms: e.g. Xm+1 = XiXj
- In all cases, we can add Xm+1,...,Xm+k to the list of original variables and perform the linear regression.
### Order-nth fit
- Order-1 polynomial: Ex: y = 1.6x + 1.05
- Order-2 polynomial: Ex: y = 0.68x^2 + 1.74x + 0.73
- Constructing a curve that has the best fit to a series of data points, subject to constraints. The n value is the number of points to which the curve will be fitted. 
### Overfitting/Generalisation
- We can find a model that explains perfectly, the training data, but does not generalise well to new data.
#### Overfitting: 
- We can do really well in the training data, but it doesn't generalise to unseen data. We don't want to just minimize error on the training set. We want to minimize the true error and develop models that generalise well to unseen data. 
- Every model has a true error measured on all possible data items we could ever encounter, but we don't have all possible data. In order to decide what is a good model, we measure error over the training set. 
- Example: Suppose we compare 2 models F and G trained using the same algorithm, assume F has a lower error on the training set and G has lower true error, then our algorithm is overfitting.
#### Generalisation:
- In ML, our goal is to develop models that generalise.
- Overly simple model:
    - High training error and high test error.
- Overly complex model:
    - Low training error but high test error.
### Uses of data
- Paritioning the data into 3 sets help to evaluate the data.
#### Training set
- Used to fit a model (find the best hypothesis in the class; learn parameters)
#### Validation set
- Used for model selection, i.e., to estimate true error and compare hypothesis classes. (E.g., compare different order polynomials). 
- Validation set is used to compare different models during development.
    - Compare hypothesis/model classes. E.g., should I use a first- or second-order polynomial fit?
- Compare hyperparameters (i.e., a parameter that is not learned but that could impact performance). E.g., what learning rate should I use?
#### Test set
- What you report the final accuracy on.
### K-fold cross validation
- Instead of just one validation set, we can evaluate on many splits!
    - Consider k partitions of the training/non-test data (usually of equal size).
    - Train with k-1 subsets, validate on kth subset. Repeat k times.
    - Average the prediction error over the k rounds/folds.
### Cross validation for comparing models
- Say we have 2 models and this is a 5-fold cross validation
- Model/hypothesis A: Linear regression with basic features 
    - ErrA = 1/5 âˆ‘j=1:5 Err^A_I (Average error of model A across all folds.)
- Model/hypothesis B: Linear regression with quadratic expansions of features
    - ErrB = 1/5 âˆ‘j=1:5 Err^B_I (Average error of model B across all folds.)
- If ErrA < ErrB then we should use Model A, we should use a linear regression model with basic features.  
- Then to get the final Model A weights, we train on the entire dataset, ignoring the cross validation folds.
### Leave one out cross validation
- Let k = n, the size of the training set
- For each model / hyperparameter setting,
    - Repeat n times:
        - Set aside one instance (xi, yi) from the training set.
        - Use all other data points to find w (optimization).
        - Measure prediction error on the held-out (xi, yi).
    - Average the prediction error over all n subsets.
- Choose the setting with lowest estimated true prediction error.

## Linear Classification
### Classification Problems
- Y is a finite discrete set
- Given data set D = (Xi, Yi), with Yi, find a hypothesis which best fits the data.
    - If Yi = (0,1), this is a binary classification.
    - If Yi can take more than two values, the problems is called multi-class classifications.
### Classification via linear regression
- Example: A tumor classifier.
    - Input: X - Tumor size
    - Binary output:
        - Y = {No recurrence = 0; Recurrence = 1}
    - yHatn = Xn (X^T * X)^-1 * X^T * Y
    - How to get a binary output?
        - Threshold: If yHat > threshold then norecurrence class, if yHat <= then Recurrence class.
        - Interpret output as probability: yHat = Probability (Recurrence)
### Probabilistic 
- Estimate P(y | x), the conditional probability of the target variable given the feature data.
### Discriminative Learning
- Partition the feature space into different regions, and classify points based on the region where they lie.
## Logistic Regression
### Probabilistic view of discriminative learning
- Suppose we have 2 classes: y = (0,1)
- What is the probability of class y = 1 given input X?
- Logistic Function = 1/ 1+exp(-a)
- Log-odds ratio = ln ((P(y = 1 | x))/ P(y = 0 | x))
### Discriminative learning
- Model the boundary between the different classes.
- a = Log-odds ratio = W0 + W1X1 + ... WmXm
- The decision boundary is the set of points for which the linear model predicts zero. a = 0.
    - In fact, we model decision boundary with linear model.
    - a = 0 means Class 1 is equally likely as Class 2.
    - If a > 0, Class 1 is more likely than Class 2.
    - If a < 0, Class 2 is more likely  than Class 1.
    - pHat(y = 1 | x) = ğœ•(w^T * X) = 1 / (1 + e^-W^T*X)
    - Logistic regression has a decision boundary that is linear in X.
### Learning the weights
- For y = (0,1), the likelihood function is
    - L(D) = P(y1,...,yn|x1,...,xn,w)
    - The likelihood of the data L(D) = Probability of correctly classifying training the data given the model parameters
        - If Yi = 1, = ğœ•(W^T * Xi)
        - If Yi = 0, = 1 - ğœ•(W^T * Xi)
### Maximizing likelihood
- Our goal is to maximize the likelihood, we want to find the parameters than give the highest likelihood.
- Likelihood: L(D) =  âˆ‘i=1:n ğœ•(W^T * Xi)^Yi (1 - ğœ•(W^T * Xi))^ 1 - Yi
    - Problem: Taking products of lots of small numbers is numerically unstable, making this function hard to optimise.
- Log likelihood: L(D) = ln(L(D)) = âˆ‘i=1:n Yi * ln(ğœ•(w^T * Xi)) + (1 - Yi) * ln(1 - ğœ•(w^T * Xi))
- The negative log-likelihood of the logistic function is known as the cross-entropy loss.
- cross-entropy(D) = - âˆ‘i=1:n Yi * ln(ğœ•(w^T * Xi)) + (1 - Yi) * ln(1 - ğœ•(w^T * Xi))
    - Basic idea: It measures how many bits of information we would need to correct the errors made by our model. 
- In cross validation, look at error metrics on the validation set, not loss.
### Gradient Descent
- ğœ•Err(W)/ğœ•w = - âˆ‘i=1:n Xi (Yi - ğœ•(w^T * Xi))
    - Yi = True output.
    - ğœ•(w^T * Xi) = Predicted output.
- Update rule: Wk+1 = Wk + aK * âˆ‘i=1:n Xi (Yi - ğœ•(w^T * Xi))
- Move in a direction that makes our prediction better.
- The algorithm:
- Given an initial weight vector w0,
- Do for k=0,1,2,...
    - Calculate T = ğœ•Err(W)/ğœ•w = - âˆ‘i=1:n Xi (Yi - ğœ•(w^T * Xi))
    - wK+1 = wK - aK * T (Take a step in the negative direction specified by the gradient.)
- End when ||wK+1 - wK|| < E
### Classification of a new observation
- After estimating w using gradient descent, calculate probabilities for a given new feature vector Xnew as follows:
    - p(yNew = 1 | xNew) = ğœ•(w^T * xNew)
    - p(yNew = 0 | xNew) = 1 - ğœ•(w^T * xNew)
- If p(yNew = 1 | xNew) > p(yNew = 0 | xNew) then yNew = 1 (xNew belongs to class 1)
- If p(yNew = 0 | xNew) > p(yNew = 1 | xNew) then yNew = 0 (xNew belongs to class 0)
### Generative learning
- Model the distribution of the different class.
- Separately model P(x|y) and P(y). Use Bayes' rule, to estimate P(y|x):
- P(y = 1|x) = (P(x|y = 1) * P(y = 1)) / P(x)
    - P(y = 1 | x) (The conditional probability of the target class (Our prediction))
    - P(x | y = 1) (How likely are we to see the observed features if the point was from class 1?)
        - Usually hard to estimate.
    - P(y = 1) (What is the marginal probability of this class? Ignoring the features, how likely are we to see class 1?)
    - P(x) (What is the marginal probability of the observed features? This is independent of the class.)
    - P(y) (Marginal probability of the class wihthot considering the features)
        - Usually easy to estimate.
- Example from spam classification: 
    - P(y = 1) = 0.01 -> In general, 1% of emails are spam.
    - P(x|y = 1) = 50% -> 50% chance of observed features occurring in a spam email.
    - P(x) = 0.10 -> 10% chance of seeing the observed features in a random email
    - P(y = 1|x) = 0.05 -> 5% chance of the email being spam.
- Why is P(x|y) and P(y) modeled seperately in Generative learning?
    - It gives extra flexibility. In the context of the spam email, suppose the spammers become more active with up to 20% of emails being spam. All that is needed to be done is modifying P(y).
    - Modeling P(x|y) allows us to make structural assumptions about the data generating process.
    - Generative models work well with smaller datasets.
### Linear Discriminant Analysis
- Linear Discriminant Analysis is a dimensionality reduction technique.     
    - LDA reduces the number of dimensions in a dataset while retaining as much information as possible. 
    - LDA uses the information from all features to create a new axis and projects the data on to the new axis in such a way as to minimizes the variance and maximizes the distance between the means of the two classes.
- LDA makes Gaussian assumptions about P(x|y)
- LDA is used for classification.
- The number of parameters to estimate in LDA is more than the number of parameters in Logistic Regression.
- LDA makes the assumptions that your data is Gaussian and that each attribute has the same variance. 
- Every class is assumed to be a Gaussian/normally distributed cluster of data points. 
- P(x|y=0) and P(x|y=1) are assumed to have the same covariance matrix. 
- LDA supports binary and multi-class classifications
#### Limations of Logistic Regression
- Two-Class Problems: Logistic regression is intended for two-classes or binary classification problems.
- Unstable with few examples / seperated classes.
### Higher-order features
- To get more flexible (non-linear) decision boundaries. Use higher-order features.
- X1,X2 are linear.
- X1,X2,X1X2,X1^2, X2^2 are curved and give a more flexible decision boundary.  
### Quadratic Discriminant Analysis
- LDA assumes all classes have the same covariance matrix.
- In QDA, each class uses its own estimate of variance or covariance where there are multiple input variables
- QDA allows different covariance matrices for each class k.
- QDA has more parameters to estimate, but greater flexibility to estimate the target function with a risk of overfitting. 
- QDA is more accurate than LDA. 
- LDA is faster than QDA due to the complex matrix operations and training time. 
- LDA is easier to analyse. 
## Naive Bayes
- In NB, assume that all of the columns of the dataset (Xj , j=1,...,m) are conditionally independent given y.
    - P(Xj | y) = P(Xj | y, Xk) for all j,k.
- Generative model structure for a data point x
    - P(x|y) = P(x1,x2,...,xm|y) = P(x1|y)P(x2|y)...P(Xm|y)
### Training
- Training a Naive Bayes classifier entails maximizing the log-likelihood function
- Likelihood: L(D) = P(Y1,Y2,...Yn|X1,X2,...,Xn)
    - Because Samples i=1:n are independent so we take product over n.
    - Input features are independent.
### Bernoulli
- A variant of Naive Bayes
- Prediction for a new input data x using log-odds ratio:
    - a(x) = log(P(y=1|x)/P(y=0|x)) = log((P(x|y=1)P(y=1)/P(x|y=0)P(y=0)))
    - If a(x) > 0, then classify as 1, If a(x) < 0, then classify as 0.
    - a(x) gives the decision boundary. 
        - Since this decision boundary corresponds to the log-odds, we can calso compute the class probability
        - Has a linear decision boundary.
### Laplace Smoothing
- Used in text classification to handle words that aren't observed in the training data, as running the maximum likelihood Naive Bayes model would result in the test document with the previously unseen word having a probability of 0.
- Replace the maximum likelihood estimate
    - ğ›³j,1 = P(Xj|y=1) = (number of instances with Xj = 1 and Y = 1) / (number of examples with Y = 1) with ğ›³j,1 = P(Xj|y=1) = ((number of instance with Xj = 1 and Y=1) + 1)/((Number of examples with y=1)+2)
    - If no example from that class, it reduces to a prior probability Pr=1/2.
    - If all examples have Xj = 1, then P(Xj = 0|y) has P = 1 / (# of examples + 2)
    - If a word appears frequently, the new estimate is only slightly biased.
    - You should laplace smooth both P(Xj |y=1) = ğ›³j,1 and P(Xj | y=0) = ğ›³j,0
## Evaluation, Bias-Variance, and Regularization
### Preprocessing raw data
- Normalisation across different features (z-score)
    - Centering and scaling with Xj^' = (ğ‘¥ğ‘— â€“ ğœ‡ğ‘—) / 6j
### Multi-class classification
- For a k-way classification problem, there are generally 2 options
    - Option 1: Learn a single classifier thaqt can produce k distinct output values.
        -   For Naive Bayes, compute P(y|x) for each class and select the class with the highest probability (See assignment 2 and the subreddit classification)
    - Option 2: Learn k different 1-vs-all binary classifiers
        - Applies to all binary classifiers, so more flexible, but often slower and creates an imbalance problem as the target class has relatively fewer data points, compared to the aggregation of the other classes.
### Performance
- Not all errors have equal impact.
- Typical classification errors:
    - Example: Consider the spam email classifier
        - A message that is not spam is assigned to the spam folder (Type 1 error or false positive)
        - A message that is spam appears in the regular folder (Type 2 error or false negative)
### Terminology
- Type of classification outputs:
    - True positive (m11): Example of class 1 predicted as class 1.
    - True negative (m00): Example of class 0 predicted as class 0.
    - False positive (m01): Example of class 0 predicted as class 1. Type I error.
    - False negative (m10): Example of class 1 predicted as class 0. Type II error.
- Total number of instances:
      - ntest = m00 + m01 + m10 + m11
### Common measures
- Accuracy = (TP + TN) / (TP + FP + FN + TN)
    - Error = 1 - Accuracy
- Precision = TP/(TP + FP) Total number of declared positives
- Recall = TP / (TP + FN) Total number of actual positives
- Sensitivity is the same as recall.
- Specificity = TN / (FP + TN) Total number of actual negatives
- False Positive Rate = FP / (FP + TN)
- True Positive is the same as recall.
- F1 measure: F = 2 ((Precision * Recall)/ (Precision + Recall)) (F is harmonic mean of precision and recall.)
### Receiver operating characteristics
- Often have a trade-off between false positives and false negatives.
    - Note: True Positive Rate = 1 â€“ False Negative rates.
    - It is common to plot ROC curves as TPR versus FPR. 
- To build the ROC curve:
    1. Train a classifier.
    2. Vary the decision boundary threshold.
    3. Compute FP rate and TP rate for different decision boundaries associated to the thresholds.
### Area under curve
- To compare 2 algorithms over a range of classification thresholds, consider the Area Under the (ROC) Curve (AUC).
    - A perfect algorithm has AUC=1.
    - A random algorithm has AUC=0.5.
    - Higher AUC doesnâ€™t mean all performance measures are better.
### Bias and variance in Machine learning
- Training Set error vs. Validation set error 
- 1%           10%          High variance
- 15%          16%          High bias
- 15%          25%          High bias and high variance
- 1%            2%          Low bias and low variance

- There are two major sources of error in machine learning
    - Bias and variance.
- In machine learning, we informally think of
    - The difference between training and validation error as algorithmâ€™s variance.
    - The difference between the Bayes error and the train error as algorithmâ€™s bias.
        - Bayes error is the lowest possible prediction error that can be achieved.
- The field of statistics has more formal definitions of bias and variance.
    - If your error metric is mean squared error
        - You can write down formulas specifying these two quantities, and prove that 
            - Total Error = Bias + Variance.
        - But we donâ€™t use this. The more informal definition of bias and variance given above will suffice.\
- High bias (Training data performance): Try a more complex model.
- High variance (Validation set performance): Try a simpler model, get more data, or regularization.
### Ridge Regression (L2-Regularisation)
- In L2-Regularisation, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients.
- Ridge regression shrinks the coefficients.
- The lower the constraint (value of Î») on the features, the more the model will resemble a linear regression model.
- Constrains the weights by imposing a penalty on their size:
    - wHatRidge = argminW    { âˆ‘i=1:n(Yi - w^T * Xi)^2 + Î»âˆ‘j=0:m Wj^2}
- The regularisation parameter Î» can be selected manually or by cross validation.
- Err(w) = âˆ‘i=1:n(Yi - w^T * Xi)^2 + Î»(||w||)^2
- Can also simply add the penalty to gradient descent
    - âˆ‚Err(w)/âˆ‚w = 2( (X^T * X * w) - (X^T * Y) ) + 2 Î» w
    - 2( (X^T * X * w) (Usual gradient for linear regression)
    - 2 Î» w (Gradient from the L2-penalty)
- If you train a ML model with L2-regularisation. The training error rate is 15% and the test error rate is 30%. You want to reduce the variance. You should increase Î».
### Lasso Regression (L1-Regularisation)
- Constrains the weights by penalizing the absolute value of their size:
    - wHatLasso = argminW  { âˆ‘i=1:n(Yi - w^T * Xi)^2 + Î»âˆ‘j=0:m |Wj|}
- Err(w) = âˆ‘i=1:n(Yi - w^T * Xi)^2 + Î»(||w||)
- Can also simply add the penalty to gradient descent
    - âˆ‚Err(w)/âˆ‚w = 2( (X^T * X * w) - (X^T * Y) ) + Î» sign(w)
    - 2( (X^T * X * w) (Usual gradient for linear regression)
    - Î» sign(w) (Gradient from the L1-penalty term sign(x)=1 if x>0, sign(x)=-1 if x < 0, and sign(x) = 0 if x=0)
### Ridge vs. Lasso
- The purpose of Ridge and Lasso Regression is to reduce model complexity and prevent over-fitting that results from simple linear regression
- Ridge regression will tend to lower all weights.
- Lasso will tend to set some weights to 0.
- Combining Lasso and Ridge regression is called the â€œelasticâ€ net.
- Regularization (e.g. Ridge and Lasso) decreases variance at the expense of some bias.
- Regularization reduces overfitting but can cause underfitting.
- The L2 and L1 penalties can be added to any model (e.g., logistic regression). Easy to incorporate with gradient descent.
## Decision Trees
- Linear regression: Fit a linear function from input data to output.
- Linear classification: Find a linear hyper-plane separating classes.
    - Generative assumptions:
    - LDA â€“ Multivariate Gaussian distributions with shared covariance matrices.
    - QDA - Multivariate Gaussian distributions with distinct covariance matrices.
    - NaÃ¯ve Bayes â€“ Conditional independence assumption.
- Many problems require more sophisticated models!
- Decision Tree: Classification by making a bunch of binary decisions that bucket our data together. These decisions can be represented as a tree.
- Done by partitioning the feature space into a bunch of cubes as every cube gets its own class.
- Decision Trees make predictions by recursively splitting on different features according to a tree structure.
- Example: classifying fruit as an orange or lemon based on height.
- For continuous attributes, split based on less than or greater than some threshold. Thus the input space is divided into regions with boundaries parallel to axes
### Example with Discrete Input
- What if the attributes are discrete? Example: If a person go to restaurant and if there is no table available, the goal is to predict if they decide to wait for a table or not.
- Internal nodes / test attributes
- Branching is determined by attribute value
- Leaf nodes are outputs (predictions)
### Expressiveness
- Discrete-input, discrete-output case:
    - Decision trees can express any function of the input attributes
    - Example: For Boolean functions, the truth table row â†’ path to leaf.
- Continuous-input, continuous-output case:
    - Can approximate any function arbitrarily closely
### Classification and regression
- Each path from root to a leaf denotes a region ğ‘…ğ‘Ÿ
of input space.
- Assume you built the tree:
    - Let (X1^ğ‘Ÿ, Y1^ğ‘Ÿ), ... , (Xk^ğ‘Ÿ, Yk^ğ‘Ÿ) be the training examples that fall into ğ‘…ğ‘Ÿ.
    - Classification tree:
        - Discrete output (e.g. class orange vs class lemon)
        - leaf value    Y^ğ‘Ÿ, i.e. class label of region r, typically set to the most common value in  (Y1^ğ‘Ÿ, Yk^ğ‘Ÿ)
    - Regression tree
        - Continuous output
        - leaf value Y^r typically set to the mean value in (Y1^ğ‘Ÿ,..., Yk^ğ‘Ÿ).
### How do we learn a decision tree?
- Given a set of labeled training samples:
1. If all the training samples have the same class, create a leaf with that class label and exit.
- Else
    2. Pick the â€œbestâ€ test attribute (i.e. internal node) to split the data on.
    3. Split the training set according to the value of the outcome of the test attribute.
    4. Recursively repeat steps 1 - 3 on each subset of the training data.
### Choosing a good split attribute
- The test attribute should provide information about the class label.
    - To quantify information we use uncertainty concept.
- How can we quantify uncertainty in prediction for a given leaf node?
    - All examples in leaf have the same class label: good (low uncertainty).
    - Each class has the same number of examples in leaf: bad (high uncertainty)
- Idea: Use counts at leaves to define probability distributions, and use information theory to measure uncertainty
### Quantifying Uncertainty / Entropy
- Entropy is a measure of expected â€œsurpriseâ€
    - How uncertain are we of the value of a draw from this distribution?
    - Unit = bits (based on the base of logarithm)
    - A fair coin flip has 1 bit of entropy (high uncertainty)
- â€œHigh Entropy":
    - Variable has a uniform like distribution
    - Flat histogram
    - Values sampled from it are less predictable
- â€œLow Entropyâ€
    - Distribution of variable has peaks and valleys
    - Histogram has lows and highs
    - Values sampled from it are more predictable
- Entropy of a joint distributions
    - Entropy can be defined based on the joint distribution of two random variables.
    - Example: Table below is ğ‘(ğ‘¥, ğ‘¦) where X = {Raining, Not raining} and Y = {Cloudy, Not cloudy}. (Note: here there are two random variables)
    - Raining and Cloudy (24/100), Raining and Not Cloudy (1/100), Not Raining and Cloudy (25/100), Not Cloudy and Not Raining (50/100).
        - H(X,Y) = -(24/100)log2(24/100) -(1/100)log2(1/100) -(25/100)log2(25/100) -(50/100)log2(50/100) = 1.56 bits
- Specific Conditional Entropy
    - Same Example: Table below is ğ‘(ğ‘¥, ğ‘¦) where X = {Raining, Not raining}, Y = {Cloudy, Not cloudy}
    - Raining and Cloudy (24/100), Raining and Not Cloudy (1/100), Not Raining and Cloudy (25/100), Not Cloudy and Not Raining (50/100).
    - What is the entropy of cloudiness Y, given that it is raining:
        - H(Y|X = raining) = - âˆ‘ P(Y|raining) log2 P(Y|raining)
        - -(24/25) log2 (24/25) - (1/25) log2 (1/25) = 0.24 bits
        - We used P(Y|X) = P(X,Y)/P(X) 
        - ğ»(ğ‘Œ|ğ‘‹ = ğ‘…ğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘”) = 0.24 bits
        - ğ»(ğ‘Œ|ğ‘‹ = ğ‘ğ‘œğ‘¡ ğ‘…ğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘”) = 0.91 bits
    - What is the entropy of cloudiness, given the knowledge of whether or not it is raining?
        - H(Y|X) = P(X = Raining) H(Y|X = Raining) + P(X = Not Raining) H(Y|X = Not Raining)
        - (25/100) * 0.24 + (75/100) * 0.91 = 0.75 bits
### Information gain
- Using the same example from before, Raining and Cloudy (24/100), Raining and Not Cloudy (1/100), Not Raining and Cloudy (25/100), Not Cloudy and Not Raining (50/100).
- How much information about cloudiness do we get by discovering whether it is raining?
    - IG(Y|X) = H(Y) - H(Y|X)
    - H(Y) = -(49/100)log2(49/100) - (51/100)log2(51/100) = 1
    - H(Y|X) = 0.75 bits
    - IG(Y|X) = 1 - 0.75 = 0.25 bits
    - IG(Y|X) is the information gain in Y due to X, or mutual information of Y and X.
    - If X is completely uninformative about Y: IG(Y|X) = 0
    - If X is completely informative about Y: IG(Y|X) = 1
- Information gain measures the informativeness of a variable, which is exactly what we desire in a decision tree learning phase.
- Example: Assume you have the following test node when performing fruit classification based on the fruit width.
    - What is the information gain of this split?
    - Let ğ’€ be r.v. denoting lemon (lmn) or orange (org), ğ‘© be r.v. denoting whether left (L) or right (R) split taken, and treat counts as probabilities.
- Root Entropy: H(Y) = - âˆ‘ P(Y)log2 P(Y)
    - H(Y) = -(49/149)log2(49/149) - (100/149)log2(100/149) = 0.91
- Leaves Entropy:
    - H(Y|B = L) = - (P(Y = LMN|L)log P(Y = LMN|L) + P(Y = ORG|L)log P(Y = ORG|L))
        - H(Y|B = L) = - (50/50)log (50/50) - (0/50)log (0/50) = 0
    - H(Y|B = R) = - (P(Y = LMN|R)log P(Y = LMN|R) + P(Y = ORG|R)log P(Y = ORG|R))
        - H(Y|B = R) = - (50/99)log (50/99) - (49/99)log (49/99) = 1
    - Information Gain:
        - IG(Y|X) = H(Y) - H(Y|B) = H(Y) - P(L) H(Y|B = L) + P(R) H(Y|B = R)
        - = 0.91 - ((50/149)*0 + (99/149) * 1) = 0.24 bits
### Constructing decision trees
- At each level, one must choose:
    1. Which variable to split.
    2. Possibly where to split it (i.e. threshold).
- Choose them based on how much information we would gain from the decision! (choose attribute/threshold that gives the highest information gain)
- Simple, greedy, recursive approach, builds up tree node-by-node
-  Start with empty decision tree and complete training set
    - Split on the most informative attribute (highest IG) and partition the dataset.
    - Recurse on sub-partitions.
- Possible termination condition: end if all examples in current subpartition share the same class label.
### What makes a good tree?
- Not too small:
    - Need to handle important and possibly subtle distinctions in data.
- Not too big:
    - Computational efficiency (avoid redundant, irrelevant attributes)
    - Avoid overfitting to training examples
    - Human interpretability
- We desire small trees with informative nodes near the root
### Avoiding overfitting
- Decision tree construction proceeds until all leaves are â€œpureâ€ , i.e. all examples are from the same class.
- As the tree grows, the generalization performance can start to degrade, because the algorithm is including irrelevant attributes/tests/outliers
- Objective: Remove some nodes to get better generalization.
    - Early stopping: Stop growing the tree by some criteria
        - E.g. stop when further splitting the data does not improve information gain of the validation set.
        - E.g. stop when the entropy at our leaves is less than some epsilon
    - Post pruning: Grow a full tree, then prune the tree by eliminating lower nodes that have low information gain on the validation set. (more popular approach)
### Advantages
- Provide a general representation of classification rules
    - Scaling / normalization not needed, as we use no notion of â€œdistanceâ€ between examples.
- The learned function, y = f(x), is easy to interpret.
- Fast learning algorithms.
- Good accuracy in practice â€“ many applications in industry!
## Feature construction
### Steps to solving a supervised learning problem
1. Decide what the input-output pairs are.
2. Decide how to encode inputs and outputs.
    - This defines the input space and output space.
3. Choose a class of hypotheses.
    - E.g. linear functions.
4. Choose an error function (cost function) to define best hypothesis.
    - E.g. Least-mean squares.
5. Choose an algorithm for searching through space of hypotheses
### Encoding input into a feature vector
- Raw features > Feature construction > Dimension Reduction > Predictor
### Words
- Representation of words.
    - Binary (present or absent). Think a list for every document, if a word is present in a document it is added to the corpus, and the word is represent in that dictionary by a 1.
    - Absolute frequency: Raw count of occurences.
    - Relative frequency: Relative to document length.
- Stopwords
    - Common words like â€œtheâ€, â€œofâ€, â€œaboutâ€ are unlikely to be informative about the contents of a document.
    - Standard practice is to remove them, though they can be useful (e.g., for author identification)
- Lemmatization
    - Inflectional morphology: changes to a word required by the grammar of a language
        - e.g., â€œplayingâ€ â€œplayedâ€ â€œplaysâ€
        - (Much worse in languages other than English, Chinese, Vietnamese)
    - Lemmatize to recover the canonical form; e.g., â€œplayâ€
### TF-IDF
- Term Frequency Ã— Inverse Document Frequency
- A term is important/indicative of a document if it:
    1. Appears many times in the document
    2. Is a relative rare word overall
- TF is usually just the count of the word (divided by the document length)
- IDF is a little more complicated:
- ğ¼ğ·ğ¹ (ğ‘¡, ğ¶ğ‘œğ‘Ÿğ‘ğ‘¢ğ‘ ) = log (#(Docs in Corpus))/(#(Docs with term t) + 1)
### N_grams
- Use sequences of words, instead of individual words
- e.g., â€¦ quick brown fox jumped â€¦
    - Unigrams (i.e. words)
        - quick, brown, fox, jumped
    - Bigrams
        - quick_brown, brown_fox, fox_jumped
    - Trigrams
        - quick_brown_fox, brown_fox_jumped
- Usually stop at N <= 3, unless you have lots and lots of data
### Dimension Reduction:
- Feature extraction
    - Reduce dimension by extracting new features by combining input features.
    - E.g. Principal Component Analysis (PCA)
- Feature selection
    - Reduce dimension by selecting a subset of the most relevant/informative features.
    - E.g. Variable Ranking
        - Think of features as random variables
        - Find how strong they are associated with the output prediction, remove the ones that are not highly associated.
### Feature Selection Methods
- The goal: Find the input representation that produces the best generalization error.
- Two classes of approaches:
    - Wrapper & Filter methods: Feature selection is applied as a pre-processing step.
    - Embedded methods: Feature selection is integrated in the learning (optimization) method, e.g. L1-regularization.
- Variable Ranking
    - Idea: Rank features by a scoring function defined for individual features, independently of the context of others. Choose the mâ€™ highest ranked features.
    - Pros / cons:
        - Need to select a scoring function.
        - Must select subset size (mâ€™): cross-validation
        - Simple and fast â€“ just need to compute a scoring function m times and sort m scores
### Principal Component Analysis
- Idea: Project data into a lower-dimensional sub-space, Rm â†’Rd, where d < m. 
- Consider a linear mapping, xi â†’ P^T * xi
    - xi is the original ğ‘š Ã— 1 feature vector for the i-th sample.
    - P is the compression matrix with dimension ğ‘š Ã— ğ‘‘.
        - P^Txi is d Ã— 1
    - Assume there is a decompression matrix U^mÃ—d
- Solution can be obtained by eigen-decomposition of X^T * X.
    - Eigen-decomposition of X^T * X gives eigen vectors and their corresponding eigen values .
    - Sorted in descending order by the magnitude of the eigenvalue
    - Pick the d eigen vectors corresponding to the top d eigen values.
    - P is mÃ—d matrix corresponding to the d selected eigenvectors.
    - Note: eigen vectors are orthogonal â” The columns of P are orthogonal!
- Select a value for d, using crossvalidation.
- Typically â€œcenterâ€ the examples before applying PCA (i.e., subtract the mean).
- Interpretation: First column of P is direction with maximal variance in the data; second column is the orthogonal direction with second-most variance, etc.
### Scoring functions
- Mutual information between features and output
- Think of Xj and Y as random variables.
- Mutual information between variable Xj and target Y: I(Xj,Y) = âˆ‘Xj âˆ‘Y P(Xj = xj, Y=y) log( (P(Xj = xj, Y=y))/P(Xj = xj) P(Y=y))
- Empirical estimate from data (assume discretized variables): Xj is the jth feature (The jth column of the data matrix X where X is n * m)
### Nonlinear dependencies with MI
- Mutual information identifies nonlinear relationships between variables.
### Best-subset selection
- Idea: Consider all possible subsets of the features, measure performance on a validation set, and keep the subset with the best performance.
- Pros / cons?
    - We get the best model!
    - Very expensive to compute, since there is a combinatorial number of subsets. (for 10 features there are 2^10 subsets)
- m features, 2^m-1 possible feature subsets
## Instance Based Learning
- So far, we assumed we have a dataset of labeled examples.
- From this, learn a parameter vector of a fixed size such that some error measure based on the training data is minimized.
- These methods are called parametric, and main goal is to summarize the data using the parameters.
    - Parametric methods are typically global = one set of parameters for the entire data space.
- In the test phase, we throw away our training set and only use model parameters to make a prediction. Key idea: just store all training examples < xi , yi >.
- When a query is made, compute the value of the new instance based on the values of the closest (most similar) points.
    - E.g. if I get a new email and if this new email looks identical to a spam email in my training set, I'm going to classify the new email as spam.
- Requirements:
    - A distance function.
    - How many closest points (neighbors) to look at?
    - How do we compute the value of the new point based on the existing values? (e.g. majority voting)
### Nearest neighbour
- Classification based on the nearest sample:
    - The idea: look at the nearest sample in the training set and copy its label as the class label of a new input data.
- Regression based on the nearest sample:
    - The idea: look at the nearest sample in the training set and copy its output value as the output value of a new input data.
- One-nearest Neighbour
    - Given: Training Data X, distance d on X
    - Learning: Nothing to do, just store the data
    - Prediction: For a new point Xnew, find nearest training sample Xi* = argminI d(Xi, Xnew), predict yNew = Yi*
- Nearest-neighbor does not explicitly compute decision boundaries.
### K-Nearest neighbour
- Nearest Neighbor is sensitive to noise or mis-labeled data (â€œclass noiseâ€).
- Smooth by having k nearest neighbor votes (k>1).
- Given: Training Data X, distance d on X
- Learning: Nothing to do, just store the data
- Predict: For a new point Xnew, find the k-nearest training samples to xNew, let their indices be i1,i2,...,iK. Predict y = mean or median of {Yi1, Yi2, ..., YiK} for regression. y = majority of {Yi1, Yi2, ..., YiK} for classification or empirical probability of each class.
### Limitations
- A lot of discontinuities.
- Sensitive to small variations in the input data.
### Distance-weighted NN
- Given: Training Data X, distance d on X, weighting function W: R -> R.
- Learning: Nothing to do, just store the data
- Predict: 
    - Given input xNew. 
    - For a new point Xi in the training data X, compute Wi = W(d(Xi,XNew)).
    - Predict: 
        - For regression: ynew = âˆ‘i=1:n wiyi / âˆ‘i=1:n wi
        - For classification with C classes:
            - ynew = argmax {yi1, yi2, â€¦,yiC} where yic= âˆ‘iâˆˆc wi/ âˆ‘i=1:n wi
    - The distances are weighed using weighting functions (W)
    - The wider the function W is, the more points that are far away matter.
- The pitfall of Distance-weighted NN is the computational cost.
    - Number of computations at test time per query:
    - Calculate m-dimensional Euclidian distance with n data points: O(nm)
    - Sort the distances: O(n log n)
    - This must be done for each query, which is very expensive.
    - Need to store the entire dataset in memory!
## Ensemble Methods
- The ensemble methods entail combining the outputs of different models
- Key idea: Run base learning algorithm(s) multiple times, then combine the predictions of the different learners to get a final prediction.
    - Whatâ€™s a base learning algorithm? NaÃ¯ve Bayes, LDA, Decision trees, SVMs, â€¦
- Option 1 (Bagging): Construct K independent models by training the same base learning algorithm on different subsets of the training data.
- Option 2 (Boosting): Incrementally train K models, where each successive model tries to fix the mistakes of the previous models
- Option 3 (Stacking): Train K different models and combine their output using a â€œmeta-classifierâ€.
### Bagging
- Bagging is Bootstrap aggregating.
- Bootstrapping is random sampling with replacement.
- Given dataset D:
    - Construct B bootstrap replicate of D, called Dl (l=1,.., B) which has the same number of examples, by drawing samples from D with replacement.
    - Choose a base classifier (e.g.LDA, QDA, etc.).
    - Train the B base classifiers (aka hypotheses) independently using their corresponding training set.
- Combine classifiers output using majority voting to classify test data.
- All hypotheses â„1, â€¦ , â„ğµ get to have a vote.
    - For classification: pick the majority class (or compute the empirical probability for each class).
    - For regression, average all the predictions.
- For each training point x, we have a set of estimates h1(x), â€¦, hK(x), with Kâ‰¤B (since x might not appear in some replicates).
    - The average empirical prediction of x is: Ä¥ (x) = (1/K) âˆ‘k=1:K hk (x).
    - We estimate the bias for point x as: ( y â€“ Ä¥(x) )^2. y is the true label for point x
    - We estimate the variance for point x as: (1/(K-1)) âˆ‘k=1:K (hk(x)- Ä¥(x))^2.
- In theory, bagging eliminates variance.
- In practice, bagging tends to reduce variance and increase bias.
- Use this with â€œunstableâ€ learners that have high variance, e.g., decision trees or nearest-neighbours.
### Random Forests
- Basic algorithm:
    - Use B bootstrap replicates to train B different trees.
    - Instead of using basic decision tree, at each node, pick mâ€™ features at random (mâ€™ < m).
    - Determine the best test (using information gain).
    - Recurse until the tree reaches maximum depth (no pruning).
- Comments:
    - Each tree has high variance, but the ensemble uses averaging, which reduces variance.
    - Random forests are very competitive in both classification and regression.
    - Individual tree in the random forest is built on a subset of observations.
### Extremely randomized trees
- Basic algorithm:
    - Construct B decision trees.
    - Pick mâ€™ attributes at random and pick a random test involving each attribute.
    - Evaluate all tests (using information gain metric) and pick the best one for the node.
    - Continue until a desired depth or a desired number of instances (nmin) at the leaf is reached.
- Comments:
    - Very reliable method for both classification and regression.
    - The smaller mâ€™ is, the more randomized the trees are. Small nmin means less bias and more variance, but variance is controlled by averaging over trees.
### Boosting
- Use the training set to train a simple predictor (weak classifier).
- Re-weight the training examples, putting more weight on examples that were not properly classified in the previous predictor.
- Repeat B times.
- Combine the simple hypotheses into a single, accurate predictor.
- Weak learners have high bias. By combining them, we get more expressive classifiers. Hence, boosting is a bias-reduction technique.
- Boosting is a method for improving the performance by aggregating the results of weak learners. 
- In boosting, individual weak learners are dependent on each other.
### Stacking
- Both bagging and boosting assume we have a single â€œbase learningâ€ algorithm.
- But what if we want to ensemble an arbitrary set of classifiers?
- E.g., combine the output of a naÃ¯ve Bayes, and a nearest neighbor model?
- Train B distinct base models, hk, k=1â€¦B, on the training set (xi, yi), i=1â€¦n
- Make a new training set where the new features are the predictions of the base models: (h1(xi), h2(xi), â€¦, hB(xi), yi), i=1â€¦n
    - (Can also add the original feature information, xi, to the new training set)
- Train a meta-model on this new training set.
- Stacking works best when the base models have complimentary strengths and weaknesses (i.e., different biases).
- For example: combining k-nearest neighbor models with different k-values, NaÃ¯ve Bayes, and logistic regression. Each of these models has different underlying assumptions so (hopefully) they will be complimentary. 
### Meta-model
- What is the meta-model? Any supervised model will work!
- Common choices:
    - Averaging (for regression tasks)
    - Majority vote (for classification tasks)
    - Linear regression (for regression tasks)
    - Logistic regression (for classification tasks)
### Weak Learners
- Assume we have some â€œweakâ€ binary classifiers:
    - A decision stump is a single node decision tree: xj>th (a decision stump makes a prediction based on the value of just a single feature.)
- â€œWeakâ€ means classification error using weak classifier Hi is only slightly better than random (e.g. for binary classification, error of a weak classifier is slightly less than 0.5).
- Weak learners have high bias.
### AdaBoost
- How to train each classifier
    - Input: x
    - predicted output: ğ‘¦Hat âˆˆ {âˆ’1, +1}
    - target: ğ‘¦ âˆˆ {âˆ’1, +1}
    - Weight on sample i for classifier t: ğ‘¤ğ‘–^t
    - Cost function for classifier t :
        - Wi ^ t+1 = {
            - ğ‘¤ğ‘–^ğ‘¡(exp(+ğ›¼ğ‘¡))  ğ‘–ğ‘“ ğ‘¦Hati^t â‰  yi (If i-th sample is misclassified, increase its weight. Note: 1 â‰¤ exp(Î±^t))         
            - ğ‘¤ğ‘–^ğ‘¡(exp(-ğ›¼ğ‘¡))  ğ‘–ğ‘“ ğ‘¦Hati^t = yi (If i-th sample is correctly classified, decrease its weight. Note: 0 < exp(âˆ’Î±^t) â‰¤ 1 . )
- How to make predictions using a committee of classifiers
    - Weight the binary prediction of each classifier by the quality of that classifier:
        - yHat(x) = sign (âˆ‘ t=1:B At * yHatT(x))
            - yHatT(x) âˆˆ {-1,+1}, yHatT(x) is prediction uusing the t classifier
            - At is the quality of the classifier.
- Adaboost looks for a good approximation to the log-odds ratio, within the space of functions that can be captured by a linear combination of the base classifiers.
### Bagging vs. Boosting:
- Bagging is typically faster, but may get a smaller error reduction (not by much).
- Bagging works well with â€œreasonableâ€ classifiers.
- Boosting works with very simple classifiers.
- Boosting may have a problem if a lot of the data is mislabeled, because it will focus on those examples a lot, leading to overfitting
## Neural Networks
- A simple linear classifier
    - Given a binary classification task: {xi, yi}i=1:n, yi={-1,+1}.
    - The perceptron (Rosenblatt, 1957) is a classifier of the form:
        - Hw,b(x) = sgn(w^T * X + b) = {+1  if w^T * X + b > 0, -1 otherwise}
    - The decision boundary is ğ°^âŠ¤ğ± + ğ‘ = 0.
    - An example (x, y) is classified correctly if and only if: ğ‘¦ ğ’˜âŠ¤ğ’™ + ğ‘ > 0
### Perceptron: A simple linear classifier
- Single perceptron can represent linear boundaries.
- To represent non-linearly separate functions (e.g. XOR), we could use a network of stacked perceptron-like elements.
- Sigmoid provide â€œsoft thresholdâ€, whereas perceptron provides â€œhard thresholdâ€
    - It has the following nice property:
- We can derive a gradient descent rule to train:
    - One sigmoid unit -> multi-layer networks of sigmoid units.
### Feed-forward neural networks
- We are stacking simple models with sigmoid output functions.
    - (I.e., basically stacking logistic regression models)
- â€œHiddenâ€ units are the output of the sigmoid/logistic models in the stack.
- However, unlike stacking in an ensemble, we want to jointly train the entire â€œnetworkâ€.
- Hi = sgn(w^T * X + b)
- X is the input data
- Hidden units are linear function + sigmoid applied to input.
- The nonlinear function is called activation function.
- Matrix notation: We can combine the hidden units together into a vector and their weights into a matrix
- Weights (here, ğ– and ğ–ğ¨ğ®ğ­) are the model parameters to be obtained during training phase. 
- Output unit: Linear function of the hidden units followed by an output â€œactivation functionâ€, ğœ™ğ‘œğ‘¢ğ‘¡.
- The activation function on the output depends on the task (e.g., regression or classification) 
- It is possible to have multiple
output units.
- E.g., for multi-label classification, multiclass classification.
- ğ¡(ğ‘–): hidden vector corresponding to the ith hidden layer.
- ğ–(ğ‘–): weight matrix corresponding to the ith hidden layer.
- ğ›(ğ‘–): bias vector corresponding to the ith hidden layer.
- ğ–(ğ‘–) and ğ–ğ‘œğ‘¢ğ‘¡ Model parameters
- In feed-forward networks the output of units in layer j become input to the
units in layers j+1.
- No cross-connection between units in the same layer.
- No backward (â€œrecurrentâ€) connections from layers downstream
- In fully-connected networks, all units in layer j provide input to all units in layer j+1.
- In general, we have an input layer, H hidden layers, and an output layer.
- Computing the output is called running the â€œforward passâ€:
- Algorithm
    - h0 = x (Initialisation)
    - for i in 1...H:
        - h^i = sigmoid(W^i * h^i-1 + b^i) (Compute the hidden layer sequentially.)
    - yHat = (Wout * h^H + bOut) (Compute the output)
- Assume the network structure (units + connections) is given.
- The learning problem is finding a good set of weights to minimize the error at the output of the network.
- Approach: gradient descent, because the form of the hypothesis formed by the network is:
    - Differentiable! Because of the choice of sigmoid units.
    - Very complex! Hence direct computation of the optimal weights is not possible.
### Gradient descent 
#### Preliminaries for NN
- Take regression as a simple case (i.e., the y values are one-dimensional
and real-valued).
- Assume we have a fully-connected network with one hidden layer.
- We want to compute the weight update after seeing a single training example (x, y).
- We are using the squared loss:
    - J(y,yHat) = 1/2 (yHat - y)^2
### Update for the output node
- âˆ‚out * h (We can think of this as the error signal at the output node)
- wOut = wOut - âˆ (Is proportional to.) (âˆ‚J/âˆ‚wOut)
### Update for the hidden node
- The error at the hidden node is a function of the error at the output and we are propagating this error backwards through the network.
- = âˆ‚hj * x (We can think of this as the error signal at the hidden node)
- Wj = Wj - âˆ (âˆ‚J/âˆ‚wOut)
### Stochastic gradient descent
- Initialize all weights and ğ‘s to small random numbers.  (Initialisation)
- Repeat until convergence:
    - Pick a training example, x. Feed example through network to compute output yHat. (Forward pass)
    - For the output unit, compute the correction:   (âˆ‚J/âˆ‚wOut) = âˆ‚OutX      âˆ‚Out = yHat - y   (Backpropagation 1)
    - For each hidden unit j, compute its share of the correction: (âˆ‚J/âˆ‚wJ) = âˆ‚OutWOut,j(sigmoid(Wj^T * X + b)(1 - sigmoid(Wj^T * X + b))x)     (Backpropagation 2)
    - Update each network weight: Wj = Wj - âˆ(âˆ‚J/âˆ‚wOut), wOut = wOut - âˆ(âˆ‚J/âˆ‚wOut)
### Organizing training data
- Stochastic gradient descent: Compute error on a single example at a time (as in previous slide).
- Batch gradient descent: Compute error on all examples.
    - Feedforward all training data, compute the corresponding gradients.
    - Take the average of the gradients of all the training examples.
    - Use the mean gradients to update weights.
    - repeat.
- Mini-batch gradient descent: Compute error on small subset.
    - Randomly select a â€œmini-batchâ€ (i.e. subset of training examples).
    - Calculate error on mini-batch, apply to update weights, and repeat.
### Activation Functions
#### Sigmoid
- The sigmoid activation is the â€œclassicâ€/ â€œoriginalâ€ activation function:
    - Ï†(z) = 1/(1 + e^-z)
- Easy to differentiate.
- Can often be interpreted as a probability.
- Easily â€œsaturatesâ€, i.e., for inputs outside the range [-4, 4] it is essentially constant, which can make it hard to train
#### Tanh
-  The tanh activation is another popular and traditional activation function:
    - Ï†(z) = tan h z = (e^z - e^-z) / (e^z + e^-z)
- Easy to differentiate:
    - âˆ‚tanh(z)/âˆ‚z = 1 - tanh^2(z)
- Can often be interpreted as a probability.
- Slightly less prone to â€œsaturationâ€ than the sigmoid but still has a fixed range
### ReLu
- Rectified Linear Unit (ReLU) is the de facto standard in deep learning:
    - ReLU(z) = max(z,0)
- Unbounded range so it never saturates (i.e., activation can increase indefinitely).
- Very strong empirical results.
### Regularisation
- We can combat overfitting in neural networks by adding regularization.
- Standard approach is to apply L2-regularization:
- Jreg = J + Î» âˆ‘ i=1:H ||W^i||^2
    - Jreg (Regularised loss)
    - J (Original loss)
    - Î» âˆ‘ i=1:H ||W^i||^2 (Frobenius norm)
- As Î» increases in value, the diagram gets less pointy and squiggly and becomes more of a straighter line.
### Compute gradients
- (Bad) Idea: Derive derivatives on paper.
    - Problem: Very tedious. Lots of matrix calculus, need lots of paper.
    - Problem: Need to recompute every time we modify the architectureâ€¦
    - Lots of room for minor bugs in the code.
    - Problem: Not feasible for very complex models!
- Better Idea: Computational graphs + Backpropagation
### Overtraining
### Choosing the learning rate
### Adaptive optimization algorithms
### Convolutional Neural Networks
### NNs to CNNs
### Convolution as Dot Product
### Detecting virtual edges
### Zero padding
### Stride
### Convolution on RGB
### Two Filters
### Pooling
### Dropout Regularisation
### Softmax
### LeNet-5
### AlexNet
### VGG-16
### Recurrent Neural Networks
### Motivation
### Forward Pass
### Backpropagation
### Long-term dependencies
### Problem of long-term dependencies
### Long short-term memory (LSTM) units
### Bidirectional RNN
